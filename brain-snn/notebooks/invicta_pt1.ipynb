{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INVICTA Spring School\n",
        "# Spiking neurons from scratch\n",
        "### Written by Jason Eshraghian\n",
        "\n",
        "What you will learn:\n",
        "* How to code and simulate a leaky integrate-and-fire neuron\n",
        "* Parameterizing a leaky integrate-and-fire neuron\n",
        "\n",
        "\n",
        "Install the latest PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
      ],
      "metadata": {
        "id": "8R08u5xKHBv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch --quiet # install snntorch"
      ],
      "metadata": {
        "id": "w8BKYFRf0Gra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "from snntorch import spikeplot as splt\n",
        "from snntorch import spikegen\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kh4NW-mc0JaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting Settings\n",
        "def plot_mem(mem, title=False):\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "  plt.plot(mem)\n",
        "  plt.xlabel(\"Time step\")\n",
        "  plt.ylabel(\"Membrane Potential\")\n",
        "  plt.xlim([0, 50])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.show()\n",
        "\n",
        "def plot_step_current_response(cur_in, mem_rec, vline1):\n",
        "  fig, ax = plt.subplots(2, figsize=(8,6),sharex=True)\n",
        "\n",
        "  # Plot input current\n",
        "  ax[0].plot(cur_in, c=\"tab:orange\")\n",
        "  ax[0].set_ylim([0, 0.2])\n",
        "  ax[0].set_ylabel(\"Input Current ($I_{in}$)\")\n",
        "  ax[0].set_title(\"Lapicque's Neuron Model With Step Input\")\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem_rec)\n",
        "  ax[1].set_ylim([0, 0.6])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "\n",
        "  if vline1:\n",
        "    ax[1].axvline(x=vline1, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_current_pulse_response(cur_in, mem_rec, title, vline1=False, vline2=False, ylim_max1=False):\n",
        "\n",
        "  fig, ax = plt.subplots(2, figsize=(8,6),sharex=True)\n",
        "\n",
        "  # Plot input current\n",
        "  ax[0].plot(cur_in, c=\"tab:orange\")\n",
        "  if not ylim_max1:\n",
        "    ax[0].set_ylim([0, 0.2])\n",
        "  else:\n",
        "    ax[0].set_ylim([0, ylim_max1])\n",
        "  ax[0].set_ylabel(\"Input Current ($I_{in}$)\")\n",
        "  ax[0].set_title(title)\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem_rec)\n",
        "  ax[1].set_ylim([0, 1])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "\n",
        "  if vline1:\n",
        "    ax[1].axvline(x=vline1, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  if vline2:\n",
        "    ax[1].axvline(x=vline2, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def compare_plots(cur1, cur2, cur3, mem1, mem2, mem3, vline1, vline2, vline3, vline4, title):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(2, figsize=(8,6),sharex=True)\n",
        "\n",
        "  # Plot input current\n",
        "  ax[0].plot(cur1)\n",
        "  ax[0].plot(cur2)\n",
        "  ax[0].plot(cur3)\n",
        "  ax[0].set_ylim([0, 0.2])\n",
        "  ax[0].set_ylabel(\"Input Current ($I_{in}$)\")\n",
        "  ax[0].set_title(title)\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem1)\n",
        "  ax[1].plot(mem2)\n",
        "  ax[1].plot(mem3)\n",
        "  ax[1].set_ylim([0, 1])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "\n",
        "  ax[1].axvline(x=vline1, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  ax[1].axvline(x=vline2, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  ax[1].axvline(x=vline3, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  ax[1].axvline(x=vline4, ymin=0, ymax=2.2, alpha = 0.25, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_cur_mem_spk(cur, mem, spk, thr_line=False, vline=False, title=False, ylim_max1=1.25, ylim_max2=1.25):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(3, figsize=(8,6), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
        "\n",
        "  # Plot input current\n",
        "  ax[0].plot(cur, c=\"tab:orange\")\n",
        "  ax[0].set_ylim([0, ylim_max1])\n",
        "  ax[0].set_xlim([0, 200])\n",
        "  ax[0].set_ylabel(\"Input Current ($I_{in}$)\")\n",
        "  if title:\n",
        "    ax[0].set_title(title)\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem)\n",
        "  ax[1].set_ylim([0, ylim_max2])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "  if thr_line:\n",
        "    ax[1].axhline(y=thr_line, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  # Plot output spike using spikeplot\n",
        "  splt.raster(spk, ax[2], s=400, c=\"black\", marker=\"|\")\n",
        "  if vline:\n",
        "    ax[2].axvline(x=vline, ymin=0, ymax=6.75, alpha = 0.15, linestyle=\"dashed\", c=\"black\", linewidth=2, zorder=0, clip_on=False)\n",
        "  plt.ylabel(\"Output spikes\")\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def plot_spk_mem_spk(spk_in, mem, spk_out, title):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(3, figsize=(8,6), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [0.4, 1, 0.4]})\n",
        "\n",
        "  # Plot input current\n",
        "  splt.raster(spk_in, ax[0], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[0].set_ylabel(\"Input Spikes\")\n",
        "  ax[0].set_title(title)\n",
        "  plt.yticks([])\n",
        "\n",
        "  # Plot membrane potential\n",
        "  ax[1].plot(mem)\n",
        "  ax[1].set_ylim([0, 1])\n",
        "  ax[1].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "  ax[1].axhline(y=0.5, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "  plt.xlabel(\"Time step\")\n",
        "\n",
        "  # Plot output spike using spikeplot\n",
        "  splt.raster(spk_rec, ax[2], s=400, c=\"black\", marker=\"|\")\n",
        "  plt.ylabel(\"Output spikes\")\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_reset_comparison(spk_in, mem_rec, spk_rec, mem_rec0, spk_rec0):\n",
        "  # Generate Plots to Compare Reset Mechanisms\n",
        "  fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(10,6), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [0.4, 1, 0.4], 'wspace':0.05})\n",
        "\n",
        "  # Reset by Subtraction: input spikes\n",
        "  splt.raster(spk_in, ax[0][0], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[0][0].set_ylabel(\"Input Spikes\")\n",
        "  ax[0][0].set_title(\"Reset by Subtraction\")\n",
        "  ax[0][0].set_yticks([])\n",
        "\n",
        "  # Reset by Subtraction: membrane potential\n",
        "  ax[1][0].plot(mem_rec)\n",
        "  ax[1][0].set_ylim([0, 0.7])\n",
        "  ax[1][0].set_ylabel(\"Membrane Potential ($U_{mem}$)\")\n",
        "  ax[1][0].axhline(y=0.5, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "\n",
        "  # Reset by Subtraction: output spikes\n",
        "  splt.raster(spk_rec, ax[2][0], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[2][0].set_yticks([])\n",
        "  ax[2][0].set_xlabel(\"Time step\")\n",
        "  ax[2][0].set_ylabel(\"Output Spikes\")\n",
        "\n",
        "  # Reset to Zero: input spikes\n",
        "  splt.raster(spk_in, ax[0][1], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[0][1].set_title(\"Reset to Zero\")\n",
        "  ax[0][1].set_yticks([])\n",
        "\n",
        "  # Reset to Zero: membrane potential\n",
        "  ax[1][1].plot(mem_rec0)\n",
        "  ax[1][1].set_ylim([0, 0.7])\n",
        "  ax[1][1].axhline(y=0.5, alpha=0.25, linestyle=\"dashed\", c=\"black\", linewidth=2)\n",
        "  ax[1][1].set_yticks([])\n",
        "  ax[2][1].set_xlabel(\"Time step\")\n",
        "\n",
        "  # Reset to Zero: output spikes\n",
        "  splt.raster(spk_rec0, ax[2][1], s=400, c=\"black\", marker=\"|\")\n",
        "  ax[2][1].set_yticks([])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_snn_spikes(spk_in, spk1_rec, spk2_rec, title):\n",
        "  # Generate Plots\n",
        "  fig, ax = plt.subplots(3, figsize=(8,7), sharex=True,\n",
        "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
        "\n",
        "  # Plot input spikes\n",
        "  splt.raster(spk_in[:,0], ax[0], s=0.03, c=\"black\")\n",
        "  ax[0].set_ylabel(\"Input Spikes\")\n",
        "  ax[0].set_title(title)\n",
        "\n",
        "  # Plot hidden layer spikes\n",
        "  splt.raster(spk1_rec.reshape(num_steps, -1), ax[1], s = 0.05, c=\"black\")\n",
        "  ax[1].set_ylabel(\"Hidden Layer\")\n",
        "\n",
        "  # Plot output spikes\n",
        "  splt.raster(spk2_rec.reshape(num_steps, -1), ax[2], c=\"black\", marker=\"|\")\n",
        "  ax[2].set_ylabel(\"Output Spikes\")\n",
        "  ax[2].set_ylim([0, 10])\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3W11TY0F0Ovz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The Leaky Integrate-and-Fire Neuron\n",
        "\n",
        "In class, we modelled a leaky integrate-and-fire neuron with the following ODE:\n",
        "\n",
        "$$ \\tau \\frac{dU_{\\rm mem}(t)}{dt} = -U_{\\rm mem}(t) + RI_{\\rm in}(t)$$\n",
        "\n",
        "where $\\tau = RC$ is the time-constant of the passive membrane. We then found the following approximate solution to this equation using the Forward-Euler method\n",
        "\n",
        "$$U(t+\\Delta t) = U(t) + \\frac{\\Delta t}{\\tau}\\big(-U(t) + RI_{\\rm in}(t)\\big)$$\n",
        "\n",
        "If you'd like to go step-by-step through that derivation, check out this more detailed tutorial [here](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_2.html).\n",
        "\n",
        "The following code block basically implements this:"
      ],
      "metadata": {
        "id": "ZeDfvtJsHT9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGqdsDFtGH6L"
      },
      "outputs": [],
      "source": [
        "def leaky_integrate_neuron(U, time_step=1e-3, I=0, R=5e7, C=1e-10):\n",
        "  tau =   # set the time-constant\n",
        "  U =    # calculate U at the next step\n",
        "  return U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default values are set to $R=50 M\\Omega$ and $C=100pF$ (i.e., $\\tau=5ms$). These are quite realistic with respect to biological neurons.\n",
        "\n",
        "Now loop through this function, iterating one time step at a time.\n",
        "The membrane potential is initialized at $U=0.9 V$, with the assumption that there is no injected input current, $I_{\\rm in}=0 A$.\n",
        "The simulation is performed with a millisecond precision $\\Delta t=1\\times 10^{-3}$s."
      ],
      "metadata": {
        "id": "2bmijWJn0_M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps =   # number of time steps of simulation\n",
        "U =   # initial membrane potential\n",
        "U_trace = []  # keeps a record of U for plotting\n",
        "\n",
        "# loop over time\n",
        "for step in range(num_steps):\n",
        "  # solve and store the next membrane potential value\n",
        "\n",
        "plot_mem(U_trace, \"Leaky Neuron Model\")"
      ],
      "metadata": {
        "id": "UDAIfBpX093G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the above neuron with different current injections $I_{in}(t)$."
      ],
      "metadata": {
        "id": "D1LmT4so1uFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Simplifying the Leaky Integrate-and-Fire Neuron"
      ],
      "metadata": {
        "id": "XaKRF4Eo113-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We made a series of assumptions in-class. For a more rigorous treatment of these assumptions, see this other tutorial I spent 700 years of my life [writing](https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_3.html).\n",
        "\n",
        "$$U[t+1] = \\underbrace{\\beta U[t]}_\\text{decay} + \\underbrace{WX[t+1]}_\\text{input} - \\underbrace{S[t]U_{\\rm thr}}_\\text{reset} \\tag{10}$$\n",
        "\n",
        "This equation assumes just one input, $X[t+1]$. In practice, a single neuron will have multiple inputs:\n",
        "\n",
        "$$U[t+1] = \\underbrace{\\beta U[t]}_\\text{decay} + \\underbrace{\\sum_i W_iX_i[t+1]}_\\text{input} - \\underbrace{S[t]U_{\\rm thr}}_\\text{reset} \\tag{10}$$\n",
        "\n",
        "\n",
        "We modeled the spiking mechanism of the neuron using a threshold-shifted Heaviside operator:\n",
        "\n",
        "$$S[t] = \\begin{cases} 1, &\\text{if}~U[t] > U_{\\rm thr} \\\\\n",
        "0, &\\text{otherwise}\\end{cases} \\tag{9}$$\n",
        "\n",
        "If a spike is triggered, the membrane potential should be reset. The *reset-by-subtraction* mechanism is modeled by:\n",
        "\n",
        "\n",
        "As $W$ is a learnable parameter, and $U_{\\rm thr}$ is often just set to $1$ (though can be tuned), this leaves the decay rate $\\beta$ as the only hyperparameter left to be specified.\n",
        "\n",
        "The following neuron model is slightly different, but will have very similar dynamics:"
      ],
      "metadata": {
        "id": "1zKiDeSM1onU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_integrate_and_fire(mem, x, w, beta, threshold=1):\n",
        "  spk =   # if membrane exceeds threshold, spk=1, else, 0\n",
        "  mem =   # simplified membrane potential equation\n",
        "  return spk, mem"
      ],
      "metadata": {
        "id": "kGrNVfdJ1m0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a quick simulation to check the neuron behaves as expected:"
      ],
      "metadata": {
        "id": "k1Fzf-3S3WGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps = 100\n",
        "mem = 0.9\n",
        "mem_trace = []  # keeps a record of U for plotting\n",
        "x = 0\n",
        "\n",
        "for step in range(num_steps):\n",
        "  mem_trace.append(mem)\n",
        "  spk, mem = leaky_integrate_and_fire(mem=mem, x=x, w=0.2, beta=0.8)  # solve next step of U\n",
        "\n",
        "plot_mem(mem_trace, \"Leaky Neuron Model\")"
      ],
      "metadata": {
        "id": "mK0xRQQp3ypC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Leaky Integrate-and-Fire Neuron in snnTorch"
      ],
      "metadata": {
        "id": "wr4TTQ204ubI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wouldn't it be convenient if we could do all this leaky integrate-and-fire neuron business in one line of code?"
      ],
      "metadata": {
        "id": "dFHU61rn407X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lif1 =   # instnatiate LIF neuron in snnTorch"
      ],
      "metadata": {
        "id": "muBPATWo40pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yeah sick.\n",
        "\n",
        "The neuron model is now stored in `lif1`. To use this neuron:\n",
        "\n",
        "**Inputs**\n",
        "* `cur_in`: each element of $W\\times X[t]$ is sequentially passed as an input\n",
        "* `mem`: the previous step membrane potential, $U[t-1]$, is also passed as input.\n",
        "\n",
        "**Outputs**\n",
        "* `spk_out`: output spike $S[t]$ ('1' if there is a spike; '0' if there is no spike)\n",
        "* `mem`: membrane potential $U[t]$ of the present step\n",
        "\n",
        "These all need to be of type `torch.Tensor`. Note that here, we assume the input current has already been weighted before passing into the `snn.Leaky` neuron. This will make more sense when we construct a network-scale model.\n",
        "\n",
        "> Note: there may be tiny differences between the previous equation and the snnTorch implementation that should not effect deep learning results."
      ],
      "metadata": {
        "id": "3gikYVl445gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_steps =  # number of time steps of simulatino\n",
        "\n",
        "# Small step current input\n",
        "w=  # weighted input\n",
        "cur_in =   # create current input tensor\n",
        "mem =  # initialize membrane potential\n",
        "spk =  # initialize spike output\n",
        "\n",
        "mem_rec = []\n",
        "spk_rec = []\n",
        "\n",
        "# neuron simulation\n",
        "for step in range(num_steps):\n",
        "  # calculate spk, mem and store them\n",
        "\n",
        "\n",
        "# convert lists to tensors\n",
        "mem_rec = torch.stack(mem_rec)\n",
        "spk_rec = torch.stack(spk_rec)\n",
        "\n",
        "plot_cur_mem_spk(cur_in, mem_rec, spk_rec, thr_line=1, ylim_max1=0.5,\n",
        "                 title=\"snn.Leaky Neuron Model\")"
      ],
      "metadata": {
        "id": "e323axy244wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. A Feedforward Spiking Neural Network\n",
        "\n",
        "So far, we have only considered how a single neuron responds to input stimulus. snnTorch makes it straightforward to scale this up to a deep neural network. In this section, we will create a 3-layer fully-connected neural network of dimensions 784-1000-10. Compared to our simulations so far, each neuron will now integrate over many more incoming input spikes.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src='https://github.com/jeshraghian/snntorch/blob/master/docs/_static/img/examples/tutorial2/2_8_fcn.png?raw=true' width=\"600\">\n",
        "</center>\n",
        "\n",
        "PyTorch is used to form the connections between neurons, and snnTorch to create the neurons. First, initialize all layers."
      ],
      "metadata": {
        "id": "txmAlA_X5qH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# layer parameters\n",
        "num_inputs = 784\n",
        "num_hidden = 1000\n",
        "num_outputs = 10\n",
        "beta = 0.99\n",
        "\n",
        "# initialize layers\n",
        "fc1 =   # dense layer 1 = current injection to leaky neuron\n",
        "lif1 =  # leaky neuron 1\n",
        "fc2 =   # dense layer 2 = current injection to leaky neuron 2\n",
        "lif2 =  # leaky neuron 2"
      ],
      "metadata": {
        "id": "485P0Ffm5tAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, initialize the hidden variables and outputs of each spiking neuron.\n",
        "As networks increase in size, this becomes more tedious. The static method `init_leaky()` can be used to take care of this. All neurons in snnTorch have their own initialization methods that follow this same syntax, e.g., `init_lapicque()`. The shape of the hidden states are automatically initialized based on the input data dimensions during the first forward pass."
      ],
      "metadata": {
        "id": "Hz71tPjw5vRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize hidden states\n",
        "mem1 =\n",
        "mem2 =\n",
        "\n",
        "# record outputs\n",
        "mem2_rec = []\n",
        "spk1_rec = []\n",
        "spk2_rec = []"
      ],
      "metadata": {
        "id": "2hc27sWN5usF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an input spike train to pass to the network. There are 200 time steps to simulate across 784 input neurons, i.e., the input originally has dimensions of $200 \\times 784$. However, neural nets typically process data in minibatches. snnTorch uses time-first dimensionality:\n",
        "\n",
        "[$time \\times batch\\_size \\times feature\\_dimensions$]\n",
        "\n",
        "So 'unsqueeze' the input along `dim=1` to indicate 'one batch' of data. The dimensions of this input tensor must be 200 $\\times$ 1 $\\times$ 784:"
      ],
      "metadata": {
        "id": "64JNdJsh5xNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spk_in = spikegen.rate_conv(torch.rand((200, 784))).unsqueeze(1)\n",
        "print(f\"Dimensions of spk_in: {spk_in.size()}\")"
      ],
      "metadata": {
        "id": "X5n5Uz_650km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's finally time to run a full simulation.\n",
        "An intuitive way to think about how PyTorch and snnTorch work together is that PyTorch routes the neurons together, and snnTorch loads the results into spiking neuron models. In terms of coding up a network, these spiking neurons can be treated like time-varying activation functions.\n",
        "\n",
        "Here is a sequential account of what's going on:\n",
        "\n",
        "*  The $i^{th}$ input from `spk_in` to the $j^{th}$ neuron is weighted by the parameters initialized in `nn.Linear`: $X_{i} \\times W_{ij}$\n",
        "* This generates the input current term from Equation $(10)$, contributing to $U[t+1]$ of the spiking neuron\n",
        "* If $U[t+1] > U_{\\rm thr}$, then a spike is triggered from this neuron\n",
        "* This spike is weighted by the second layer weight, and the above process is repeated for all inputs, weights, and neurons.\n",
        "* If there is no spike, then nothing is passed to the post-synaptic neuron.\n",
        "\n",
        "The only difference from our simulations thus far is that we are now scaling the input current with a weight generated by `nn.Linear`, rather than manually setting $W$ ourselves."
      ],
      "metadata": {
        "id": "4M0HRFnj52w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# network simulation\n",
        "for step in range(num_steps):\n",
        "    cur1 =            # post-synaptic current <-- spk_in x weight\n",
        "    spk1, mem1 =      # mem[t+1] <--post-syn current + decayed membrane\n",
        "    cur2 = fc2(spk1)  # post-synaptic current: layer 2\n",
        "    spk2, mem2 = lif2(cur2, mem2) # LIF neuron layer 2\n",
        "\n",
        "    # store all variables\n",
        "    mem2_rec.append(mem2)\n",
        "    spk1_rec.append(spk1)\n",
        "    spk2_rec.append(spk2)\n",
        "\n",
        "# convert lists to tensors\n",
        "mem2_rec = torch.stack(mem2_rec)\n",
        "spk1_rec = torch.stack(spk1_rec)\n",
        "spk2_rec = torch.stack(spk2_rec)\n",
        "\n",
        "plot_snn_spikes(spk_in, spk1_rec, spk2_rec, \"Fully Connected Spiking Neural Network\")"
      ],
      "metadata": {
        "id": "QtxWv5iY54fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If you run into errors, then try re-initializing your networks and parameters.\n",
        "\n",
        "\n",
        "At this stage, the spikes don't have any real meaning. The inputs and weights are all randomly initialized, and no training has taken place. But the spikes  should appear to be propagating from the first layer through to the output. If you are not seeing any spikes, then you might have been unlucky in the weight initialization lottery - you might want to try re-running the last four code-blocks.\n",
        "\n",
        "`spikeplot.spike_count` can create a spike counter of the output layer. The following animation will take some time to generate. <br>\n",
        "\n",
        "> Note: if you are running the notebook locally on your desktop, please uncomment the line below and modify the path to your ffmpeg.exe\n",
        "\n"
      ],
      "metadata": {
        "id": "BrdJZt4W6HLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "fig, ax = plt.subplots(facecolor='w', figsize=(12, 7))\n",
        "labels=['0', '1', '2', '3', '4', '5', '6', '7', '8','9']\n",
        "spk2_rec = spk2_rec.squeeze(1).detach().cpu()\n",
        "\n",
        "# plt.rcParams['animation.ffmpeg_path'] = 'C:\\\\path\\\\to\\\\your\\\\ffmpeg.exe'\n",
        "\n",
        "#  Plot spike count histogram\n",
        "anim = splt.spike_count(spk2_rec, fig, ax, labels=labels, animate=True)\n",
        "HTML(anim.to_html5_video())\n",
        "# anim.save(\"spike_bar.mp4\")"
      ],
      "metadata": {
        "id": "53LNVAX46IgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`spikeplot.traces` lets you visualize the membrane potential traces. We will plot 9 out of 10 output neurons.\n",
        "Compare it to the animation and raster plot above to see if you can match the traces to the neuron."
      ],
      "metadata": {
        "id": "KQRAH1Zz6KVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot membrane potential traces\n",
        "splt.traces(mem2_rec.squeeze(1), spk=spk2_rec.squeeze(1))\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(8, 6)"
      ],
      "metadata": {
        "id": "FWp_1uny6L_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is fairly normal if some neurons are firing while others are completely dead. Again, none of these spikes have any real meaning until the weights have been trained."
      ],
      "metadata": {
        "id": "U5UM9tt96OqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "That covers how to simplify the leaky integrate-and-fire neuron model, and then using it to build a spiking neural network. In practice, we will almost always prefer to use `snn.Leaky` over `snn.Lapicque` for training networks, as there is a smaller hyperparameter search space.\n",
        "\n",
        "The next tutorial will show how to train SNNs. If you wish to get ahead, go straight to [this snnTorch notebook](https://snntorch.readthedocs.io/en/latest/tutorials/index.html)."
      ],
      "metadata": {
        "id": "3SlDUImz6Q2e"
      }
    }
  ]
}