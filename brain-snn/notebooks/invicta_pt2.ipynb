{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# INVICTA Spring School\n",
        "# Training Spiking Neural Networks\n",
        "### Written by Jason Eshraghian\n",
        "\n",
        "What you will learn:\n",
        "* Learn how spiking neurons are implemented as a recurrent network\n",
        "* Train a fully-connected SNN on the static MNIST dataset\n",
        "* Download event-based data and train a convolutional SNN with it\n",
        "\n",
        "Install the latest PyPi distribution of snnTorch by clicking into the following cell and pressing `Shift+Enter`."
      ],
      "metadata": {
        "id": "8R08u5xKHBv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch # install snntorch"
      ],
      "metadata": {
        "id": "w8BKYFRf0Gra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports"
      ],
      "metadata": {
        "id": "H_3jC4pJ8xzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import snntorch as snn\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "kh4NW-mc0JaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting up the Static MNIST Dataset\n"
      ],
      "metadata": {
        "id": "ZeDfvtJsHT9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGqdsDFtGH6L"
      },
      "outputs": [],
      "source": [
        "# dataloader arguments\n",
        "batch_size = 128\n",
        "data_path='/tmp/data/mnist'\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a transform\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((28, 28)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0,), (1,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)"
      ],
      "metadata": {
        "id": "c_RmbMFQ83iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "nNPwftY884_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define the Network"
      ],
      "metadata": {
        "id": "2bmijWJn0_M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network Architecture\n",
        "num_inputs =   # number of inputs\n",
        "num_hidden =   # number of hidden neurons\n",
        "num_outputs =  # number of classes (i.e., output neurons)\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps ="
      ],
      "metadata": {
        "id": "UDAIfBpX093G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code-block, note how the decay rate `beta` has two alternative definitions:\n",
        "* `beta1` is set to a global decay rate for all neurons in the first spiking layer.\n",
        "* `beta2` is randomly initialized to a vector of 10 different numbers. Each spiking neuron in the output layer (which not-so-coincidentally has 10 neurons) therefore has a unique, and random, decay rate."
      ],
      "metadata": {
        "id": "hH8nHOsZ9Hxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        beta1 = # global decay rate for all leaky neurons in layer 1\n",
        "        beta2 = # independent decay rate for each leaky neuron in layer 2: [0, 1)\n",
        "\n",
        "        # Initialize layers\n",
        "        self.fc1 =\n",
        "        self.lif1 = # not a learnable decay rate\n",
        "        self.fc2 =\n",
        "        self.lif2 = # learnable decay rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        mem1 =  # reset/init hidden states at t=0\n",
        "        mem2 =  # reset/init hidden states at t=0\n",
        "\n",
        "        spk2_rec = [] # record output spikes\n",
        "        mem2_rec = [] # record output hidden states\n",
        "\n",
        "        for step in range(num_steps): # loop over time\n",
        "            cur1 =\n",
        "            spk1, mem1 =\n",
        "            cur2 =\n",
        "            spk2, mem2 =\n",
        "\n",
        "            spk2_rec.append(spk2) # record spikes\n",
        "            mem2_rec.append(mem2) # record membrane\n",
        "\n",
        "        return torch.stack(spk2_rec), torch.stack(mem2_rec)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "net = Net().to(device)"
      ],
      "metadata": {
        "id": "7rEX7-U687zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in the `forward()` function will only be called once the input argument `x` is explicitly passed into `net`.\n",
        "\n",
        "* `fc1` applies a linear transformation to all input pixels from the MNIST dataset;\n",
        "* `lif1` integrates the weighted input over time, emitting a spike if the threshold condition is met;\n",
        "* `fc2` applies a linear transformation to the output spikes of `lif1`;\n",
        "* `lif2` is another spiking neuron layer, integrating the weighted spikes over time.\n",
        "\n",
        "A 'biophysical' interpretation is that `fc1` and `fc2` generate current injections that are fed into a set of $1,000$ and $10$ spiking neurons in `lif1` and `lif2`, respectively.\n",
        "\n",
        "> Note: the number of spiking neurons is automatically inferred by the dimensionality of the dimensions of the current injection value."
      ],
      "metadata": {
        "id": "3le9vSo29ocU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training the SNN"
      ],
      "metadata": {
        "id": "XaKRF4Eo113-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Accuracy Metric\n",
        "Below is a function that takes a batch of data, counts up all the spikes from each neuron (i.e., a rate code over the simulation time), and compares the index of the highest count with the actual target. If they match, then the network correctly predicted the target."
      ],
      "metadata": {
        "id": "1zKiDeSM1onU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass data into the network, sum the spikes over time\n",
        "# and compare the neuron with the highest number of spikes\n",
        "# with the target\n",
        "\n",
        "def print_batch_accuracy(data, targets, train=False):\n",
        "    output, _ = net(data.view(batch_size, -1))\n",
        "    _, idx = output.sum(dim=0).max(1)\n",
        "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
        "\n",
        "    if train:\n",
        "        print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
        "\n",
        "def train_printer(\n",
        "    data, targets, epoch,\n",
        "    counter, iter_counter,\n",
        "        loss_hist, test_loss_hist, test_data, test_targets):\n",
        "    print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
        "    print(f\"Train Set Loss: {loss_hist[counter]:.2f}\")\n",
        "    print(f\"Test Set Loss: {test_loss_hist[counter]:.2f}\")\n",
        "    print_batch_accuracy(data, targets, train=True)\n",
        "    print_batch_accuracy(test_data, test_targets, train=False)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "kGrNVfdJ1m0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Loss Definition\n",
        "The `nn.CrossEntropyLoss` function in PyTorch automatically handles taking the softmax of the output layer as well as generating a loss at the output."
      ],
      "metadata": {
        "id": "k1Fzf-3S3WGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "mK0xRQQp3ypC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Optimizer\n",
        "Adam is a robust optimizer that performs well on recurrent networks, so let's use that with a learning rate of $5\\times10^{-4}$."
      ],
      "metadata": {
        "id": "wr4TTQ204ubI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
      ],
      "metadata": {
        "id": "muBPATWo40pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 One Iteration of Training\n",
        "Take the first batch of data and load it onto CUDA if available."
      ],
      "metadata": {
        "id": "IKTBRtsX-ZNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, targets = next(iter(train_loader))\n",
        "data = data.to(device)\n",
        "targets = targets.to(device)"
      ],
      "metadata": {
        "id": "e323axy244wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flatten the input data to a vector of size $784$ and pass it into the network."
      ],
      "metadata": {
        "id": "ZZ1zApYM-cRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spk_rec, mem_rec = net(...)\n",
        "print(mem_rec.size())"
      ],
      "metadata": {
        "id": "J-w2w7ME-d2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The recording of the membrane potential is taken across:\n",
        "* 25 time steps\n",
        "* 128 samples of data\n",
        "* 10 output neurons\n",
        "\n",
        "We wish to calculate the loss at every time step, and sum these up together:\n",
        "\n",
        "\n",
        "$$\\mathcal{L}_{Total-CE} = \\sum_t\\mathcal{L}_{CE}[t]$$"
      ],
      "metadata": {
        "id": "AIikA-Rh-fSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the total loss value\n",
        "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "# sum loss at every step\n",
        "for step in range(num_steps):\n",
        "  loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "print(f\"Training loss: {loss_val.item():.3f}\")"
      ],
      "metadata": {
        "id": "WHlczZEb-vqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss is quite large, because it is summed over 25 time steps. The accuracy is also bad (it should be roughly around 10%) as the network is untrained:"
      ],
      "metadata": {
        "id": "InrjD9n--xCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_batch_accuracy(data, targets, train=True)"
      ],
      "metadata": {
        "id": "MxRBOuqw-x8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single weight update is applied to the network as follows:"
      ],
      "metadata": {
        "id": "0dvil_l3-09y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clear previously stored gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# calculate the gradients\n",
        "loss_val.backward()\n",
        "\n",
        "# weight update\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "7Okzx-3p-0Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, re-run the loss calculation and accuracy after a single iteration:"
      ],
      "metadata": {
        "id": "iHbLCKsa-3Ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate new network outputs using the same data\n",
        "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
        "\n",
        "# initialize the total loss value\n",
        "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "\n",
        "# sum loss at every step\n",
        "for step in range(num_steps):\n",
        "  loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "print(f\"Training loss: {loss_val.item():.3f}\")\n",
        "print_batch_accuracy(data, targets, train=True)"
      ],
      "metadata": {
        "id": "Cvx9Ux2--4Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After only one iteration, the loss should have decreased and accuracy should have increased. Note how membrane potential is used to calculate the cross entropy loss, and spike count is used for the measure of accuracy. It is also possible to use the spike count in the loss ([see Tutorial 6 in the snnTorch docs](https://snntorch.readthedocs.io/en/latest/tutorials/index.html))"
      ],
      "metadata": {
        "id": "7T3MCnxo-5fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Training Loop\n",
        "\n",
        "Let's combine everything into a training loop. We will train for one epoch (though feel free to increase `num_epochs`), exposing our network to each sample of data once."
      ],
      "metadata": {
        "id": "hDrQ7oMM--dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "loss_hist = []\n",
        "test_loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    iter_counter = 0\n",
        "    train_batch = iter(train_loader)\n",
        "\n",
        "    # Minibatch training loop\n",
        "    for data, targets in train_batch:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        net.train()\n",
        "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
        "\n",
        "        # initialize the loss & sum over time\n",
        "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "        for step in range(num_steps):\n",
        "            loss_val += loss(mem_rec[step], targets)\n",
        "\n",
        "        # Gradient calculation + weight update\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store loss history for future plotting\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Test set\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            test_data, test_targets = next(iter(test_loader))\n",
        "            test_data = test_data.to(device)\n",
        "            test_targets = test_targets.to(device)\n",
        "\n",
        "            # Test set forward pass\n",
        "            test_spk, test_mem = net(test_data.flatten(1))\n",
        "\n",
        "            # Test set loss\n",
        "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
        "            for step in range(num_steps):\n",
        "                test_loss += loss(test_mem[step], test_targets)\n",
        "            test_loss_hist.append(test_loss.item())\n",
        "\n",
        "            # Print train/test loss/accuracy\n",
        "            if counter % 50 == 0:\n",
        "                train_printer(\n",
        "                    data, targets, epoch,\n",
        "                    counter, iter_counter,\n",
        "                    loss_hist, test_loss_hist,\n",
        "                    test_data, test_targets)\n",
        "            counter += 1\n",
        "            iter_counter +=1"
      ],
      "metadata": {
        "id": "UCnjeX4U_AVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If this was your first time training an SNN, then congratulations. I'm proud of you and I always believed in you."
      ],
      "metadata": {
        "id": "MWXOCBNI_B47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Results\n",
        "## 5.1 Plot Training/Test Loss"
      ],
      "metadata": {
        "id": "Ll3ILBy8_Xlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Loss\n",
        "fig = plt.figure(facecolor=\"w\", figsize=(10, 5))\n",
        "plt.plot(loss_hist)\n",
        "plt.plot(test_loss_hist)\n",
        "plt.title(\"Loss Curves\")\n",
        "plt.legend([\"Train Loss\", \"Test Loss\"])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rHBc-kuZ_Z4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss curves are noisy because the losses are tracked at every iteration, rather than averaging across multiple iterations."
      ],
      "metadata": {
        "id": "fdbCbGQu_bcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Test Set Accuracy\n",
        "This function iterates over all minibatches to obtain a measure of accuracy over the full 10,000 samples in the test set."
      ],
      "metadata": {
        "id": "NsXGUAhM_csL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# drop_last switched to False to keep all samples\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "  net.eval()\n",
        "  for data, targets in test_loader:\n",
        "    data = data.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # forward pass\n",
        "    test_spk, _ = net(data.view(data.size(0), -1))\n",
        "\n",
        "    # calculate total accuracy\n",
        "    _, predicted = test_spk.sum(dim=0).max(1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
        "print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "LtVoTPkw_eub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this block for a good time\n",
        "import requests\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def display_image_from_url(url):\n",
        "    response = requests.get(url, stream=True)\n",
        "    display(Image(response.content))\n",
        "\n",
        "url = \"http://www.clker.com/cliparts/7/8/a/0/1498553633398980412very-nice-borat.med.png\"\n",
        "display_image_from_url(url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NaTLN3HN_7R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Handling Neuromorphic Data with Tonic"
      ],
      "metadata": {
        "id": "4lvCNRHbOGW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tonic --quiet"
      ],
      "metadata": {
        "id": "l5rPCjp5OIhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 PokerDVS Dataset\n",
        "\n",
        "The dataset used in this tutorial is POKERDVS by T. Serrano-Gotarredona and B. Linares-Barranco:\n",
        "\n",
        "```\n",
        "Serrano-Gotarredona, Teresa, and Bernabé Linares-Barranco. \"Poker-DVS and MNIST-DVS. Their history, how they were made, and other details.\" Frontiers in neuroscience 9 (2015): 481.\n",
        "```\n",
        "\n",
        "It is comprised of four classes, each being a suite of a playing card deck: clubs, spades, hearts, and diamonds. The data consists of 131 poker pip symbols, and was collected by flipping poker cards in front of a DVS128 camera."
      ],
      "metadata": {
        "id": "nhJunMSEOVMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tonic\n",
        "\n",
        "poker_train = tonic.datasets.POKERDVS(save_to='./data', train=True)\n",
        "poker_test = tonic.datasets.POKERDVS(save_to='./data', train=False)\n",
        "\n",
        "events, target = poker_train[0]\n",
        "print(events)\n",
        "tonic.utils.plot_event_grid(events)"
      ],
      "metadata": {
        "id": "mM4cKrtmOgPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tonic.transforms as transforms\n",
        "from tonic import DiskCachedDataset\n",
        "\n",
        "# time_window\n",
        "frame_transform = tonic.transforms.Compose([tonic.transforms.Denoise(filter_time=10000),\n",
        "                                            tonic.transforms.ToFrame(\n",
        "                                            sensor_size=tonic.datasets.POKERDVS.sensor_size,\n",
        "                                            time_window=1000)\n",
        "                                            ])\n",
        "\n",
        "batch_size = 8\n",
        "cached_trainset = DiskCachedDataset(poker_train, transform=frame_transform, cache_path='./cache/pokerdvs/train')\n",
        "cached_testset = DiskCachedDataset(poker_test, transform=frame_transform, cache_path='./cache/pokerdvs/test')\n",
        "\n",
        "train_loader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "test_loader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "data, labels = next(iter(train_loader))\n",
        "print(data.size())\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "cGgrBCjqOhpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.functional as F\n",
        "\n",
        "# Define Network\n",
        "class DVSNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        beta = 0.9\n",
        "\n",
        "        # Initialize layers\n",
        "        self.conv1  = nn.Conv2d(2, 12, 5)\n",
        "        self.mp1    = nn.MaxPool2d(2)\n",
        "        self.lif1   = snn.Leaky(beta=beta)\n",
        "        self.conv2  = nn.Conv2d(12, 32, 5)\n",
        "        self.mp2    = nn.MaxPool2d(2)\n",
        "        self.lif2   = snn.Leaky(beta=beta)\n",
        "        self.fc     = nn.Linear(32*5*5, 4)\n",
        "        self.lif3   = snn.Leaky(beta=beta)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        for step in range(x.size(0)):\n",
        "            cur1       = self.mp1(self.conv1(x[step]))\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2       = self.mp2(self.conv2(spk1))\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            cur3       = self.fc(spk2.flatten(1))\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# Load the network onto CUDA if available\n",
        "dvsnet = DVSNet().to(device)"
      ],
      "metadata": {
        "id": "ca2zsFSXOkVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, dataloader, num_epochs=1):\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "  counter = 0\n",
        "\n",
        "  # Outer training loop\n",
        "  for epoch in range(num_epochs):\n",
        "      train_batch = iter(dataloader)\n",
        "\n",
        "      # Minibatch training loop\n",
        "      for data, targets in train_batch:\n",
        "          data = data.to(device)\n",
        "          targets = targets.to(device)\n",
        "\n",
        "          # forward pass\n",
        "          model.train()\n",
        "          spk_rec, _ = model(data)\n",
        "\n",
        "          # initialize the loss & sum over time\n",
        "          loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
        "          loss_val = loss(spk_rec.sum(0), targets) # batch x num_out\n",
        "\n",
        "          # Gradient calculation + weight update\n",
        "          optimizer.zero_grad()\n",
        "          loss_val.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print train/test loss/accuracy\n",
        "          if counter % 10 == 0:\n",
        "              print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
        "          counter += 1\n",
        "\n",
        "          if counter == 100:\n",
        "            break\n",
        "\n",
        "training_loop(dvsnet, train_loader)"
      ],
      "metadata": {
        "id": "SA1D451UOqbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_accuracy(model, dataloader):\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    running_length = 0\n",
        "    running_accuracy = 0\n",
        "\n",
        "    for data, targets in iter(dataloader):\n",
        "      data = data.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward-pass\n",
        "      spk_rec, _ = model(data)\n",
        "      spike_count = spk_rec.sum(0) # batch x num_outputs\n",
        "      _, max_spike = spike_count.max(1)\n",
        "\n",
        "      # correct classes for one batch\n",
        "      num_correct = (max_spike == targets).sum()\n",
        "\n",
        "      # total accuracy\n",
        "      running_length += len(targets)\n",
        "      running_accuracy += num_correct\n",
        "\n",
        "    accuracy = (running_accuracy / running_length)\n",
        "\n",
        "    return accuracy.item()\n"
      ],
      "metadata": {
        "id": "eytnSVQ8OtBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loop(dvsnet, train_loader, num_epochs=10)\n",
        "print(f\"Test set accuracy: {measure_accuracy(dvsnet, test_loader)}\")"
      ],
      "metadata": {
        "id": "N8q1rzcNOoAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "That covers how to train a spiking neural network. There are a lot of ways to alter this, e.g., by using different neuron models, surrogate gradients, learnable beta and threshold values, or modifying the fully-connected layers by replacing them with convolutions or whatever else you fancy."
      ],
      "metadata": {
        "id": "3SlDUImz6Q2e"
      }
    }
  ]
}